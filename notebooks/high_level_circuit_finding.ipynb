{"cells":[{"cell_type":"markdown","metadata":{"id":"IVeslKqjbFZf"},"source":["# Transformer-Specific Interpretability: Mechanistic Interpretability\n","**Michael Hanna**\n","\n","This notebook will explain how to use a pre-built set of tools for playing with models' computational graphs in order to find and verify circuits! We will focus on the greater-than task, but you can use this code to find your own circuits of interest as well.\n","\n","---\n","⚠️**Before starting this notebook:**⚠️\n","- [ ] Change the runtime to GPU (`Runtime -> Change runtime type -> Hardware Accelator -> GPU`)."]},{"cell_type":"markdown","metadata":{"id":"qXn58UFrGldW"},"source":["## 0 Setup\n","Just run this code -- don't bother reading it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SbVqGa7tbFZ3"},"outputs":[],"source":["%pip install transformer-lens\n","!apt install -y graphviz\n","!apt install libgraphviz-dev\n","%pip install pygraphviz\n","%pip install cmapy\n","!wget https://raw.githubusercontent.com/hannamw/eacl-tutorial-resources/main/files/greater-than-data.csv\n","!git clone https://github.com/hannamw/EAP-positional.git\n","%cd EAP-positional\n","!git checkout tutorial\n","%pip install -e .\n","import sys, os\n","sys.path.append(os.getcwd())\n","%cd .."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPrmWV0nbFaC"},"outputs":[],"source":["# Import stuff\n","import torch\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","\n","from typing import List, Union, Optional, Tuple, Literal\n","from functools import partial\n","from IPython.display import Image, display\n","\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer, ActivationCache\n","import plotly.io as pio\n","\n","pio.renderers.default = \"colab\"\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","if not torch.cuda.is_available():\n","    print(\"WARNING: Running on CPU. Did you remember to set your Colab accelerator to GPU?\")"]},{"cell_type":"markdown","metadata":{"id":"Evm3wFrAbFaK"},"source":["## 1 Loading and Running Models\n","\n","This tutorial is built with TransformerLens, a powerful interpretability library for working with pre-trained transformer-based NLP models. TransformerLens allows you to easily interact with model representations and components at a low-level. TransformerLens is implemented in PyTorch, so models should feel similar to those you have used before. So far, it supports primarily autoregressive models (e.g., GPT-2). Models must be ported to TransformerLens before use.\n","\n","To begin, we will simply load and run a pre-trained model, `gpt2-small`. By setting `model_name` to other values, you can load other models. See other valid model names [here](https://neelnanda-io.github.io/TransformerLens/model_properties_table.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtDarERlbFaP"},"outputs":[],"source":["model_name = 'gpt2-small'\n","model = HookedTransformer.from_pretrained(model_name, device=device)\n","model.cfg.use_attn_in = True\n","model.cfg.use_split_qkv_input = True\n","model.cfg.use_attn_result = True\n","model.cfg.use_hook_mlp_in = True"]},{"cell_type":"markdown","metadata":{"id":"gkfzunIjbFaQ"},"source":["Models in TransformerLens run very similarly to HuggingFace Transformers models. Unlike HuggingFace models, however, TransformerLens' HookedTransformers bundle together models and tokenizers. You can access the tokenizer via `model.tokenizer`, but you can also call `model()` directly on strings (or on tokens), as seen below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sr37wX3LbFaU"},"outputs":[],"source":["s = \"This year's EACL takes place in St. Julian's, on the beautiful Mediterranean island of\"\n","logits = model(s).squeeze(0).cpu()\n","probs = torch.softmax(logits, dim=-1)\n","\n","# let's see what the top 5 predictions are\n","probs, next_tokens = torch.topk(probs[-1], 5)\n","print(s, \"...\")\n","for token_id, prob in zip(next_tokens, probs):\n","    token = model.tokenizer.decode(token_id.item())\n","    print(f\"{token.strip()}\\t {prob.item():.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"5PX7KXMqnIcK"},"source":["## 2 The Task: Greater-Than"]},{"cell_type":"markdown","metadata":{"id":"M9O0ke9TbFaZ"},"source":["Now that we've loaded up a model, let's look for a circuit for a specific task! In this tutorial, we'll focus on the greater-than task, which has a known and faithful circuit, but you can replace this with your own as well!"]},{"cell_type":"markdown","metadata":{"id":"3A3g5kqXnIcK"},"source":["### 2.1 Loading and viewing the data\n","For your convenience, we've prepared a pre-made dataset for this task. Each datapoint is a clean sentence, corrupted sentence, and the start year of the event; the corrupted start year is always `01`.\n","\n","In this dataset, all sentences tokenize to the same length! It's only necessary that each pair of (clean, corrupted) tokenize to the same length, but if all sentences are the same length, you can always assess your metric on the final (-1) token position, instead of taking into account input lengths."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLgdRrrSnIcL"},"outputs":[],"source":["df = pd.read_csv('greater-than-data.csv')\n","print(df.iloc[0])"]},{"cell_type":"markdown","metadata":{"id":"ZeHFxNEhnIcL"},"source":["We'll also batch this dataset for convenience."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hakweyNnIcL"},"outputs":[],"source":["def batch_dataset(df, batch_size=8):\n","    clean, corrupted, label = [df[col].tolist() for col in ['clean', 'corrupted', 'start_year']]\n","    clean = [clean[i:i+batch_size] for i in range(0, len(df), batch_size)]\n","    corrupted = [corrupted[i:i+batch_size] for i in range(0, len(df), batch_size)]\n","    label = [torch.tensor(label[i:i+batch_size]) for i in range(0, len(df), batch_size)]\n","    return [(clean[i], corrupted[i], label[i]) for i in range(len(clean))]\n","\n","dataset = batch_dataset(df, batch_size=8)"]},{"cell_type":"markdown","source":["One important note: for this procedure to work, the lengths of the clean and corrupted inputs (\"The war lasted from the 1741 to the year 17\" and \"The war lasted from the 1701 to the year 17\") must be identical post-tokenization! Otherwise, you can't directly patch one sequence's activations onto another, because their sizes won't match! We test this here."],"metadata":{"id":"YvpydY3bW22P"}},{"cell_type":"code","source":["for clean, corrupted, _ in dataset:\n","    clean_toks = model.tokenizer(clean).input_ids\n","    corrupted_toks = model.tokenizer(corrupted).input_ids\n","    for clean_example_toks, corrupted_example_toks in zip(clean_toks, corrupted_toks):\n","        assert len(clean_example_toks) == len(corrupted_example_toks), f\"Found clean/corrupted pair with different tokenized lengths: '{clean_example_toks}' and '{corrupted_example_toks}' with lengths {len(clean_example_toks)} and {len(corrupted_example_toks)}\""],"metadata":{"id":"NLnmVaRdXea3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2f_tiWuBbFab"},"source":["### 2.2 The Metric\n","We will measure performance using probability difference (`prob_diff`). This measures the probability assigned to correct years minus the probability assigned to incorrect year. So, for a sentence like \"The journey lasted from the year 1577 to the year 15\", we would measure the probability assigned to years > 77, minus the probability assigned to years <= 77."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXEfaufCnIcL"},"outputs":[],"source":["year_indices = torch.tensor([model.tokenizer(f'{year:02d}').input_ids[0] for year in range(100)])\n","\n","def prob_diff(logits: torch.Tensor, corrupted_logits, input_lengths,  labels: torch.Tensor, loss=False, mean=False):\n","    \"\"\"\n","    the probability difference metric, which takes in logits and labels (years), and\n","    returns the difference in prob. assigned to valid (> year) and invalid (<= year) tokens\n","\n","    (corrupted_logits and input_lengths are due to the Graph framework introduced below)\n","    \"\"\"\n","    probs = torch.softmax(logits[:, -1], dim=-1)\n","    probs = probs[:, year_indices]\n","\n","    results = []\n","    for prob, year in zip(probs, labels):\n","        results.append(prob[year + 1 :].sum() - prob[: year + 1].sum())\n","\n","    results = torch.stack(results)\n","    if loss:\n","        results = -results\n","    if mean:\n","        results = results.mean()\n","    return results\n","\n","metric = prob_diff"]},{"cell_type":"markdown","metadata":{"id":"SNyoAbzUnIcL"},"source":["## 3 Working with Circuits\n","\n","In this section, we'll work with `Graph`s, objects that makes circuit finding and evaluation easy. A `Graph` represents a model's computational graph. Once instantiated, it contains various `Node`s, representing mostly attention heads and MLPs, and `Edge`s representing connections between nodes. Each `Node` and `Edge` is either in the graph (circuit) or not; by default, all `Node` and `Edge` objects are included within the graph.\n","\n","Let's start by instantiating a `Graph`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2--hOBNSnIcL"},"outputs":[],"source":["import eap\n","from eap.graph import Graph\n","from eap import evaluate\n","from eap import attribute_mem as attribute"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCC2x0nPnIcM"},"outputs":[],"source":["g = Graph.from_model(model)\n","print(list(g.nodes.items())[:10])\n","print(list(g.edges.items())[:10])"]},{"cell_type":"markdown","metadata":{"id":"S2ZEeX-WnIcM"},"source":["### 3.1 What should go in our Circuit?\n","Right now everything is in our circuit, but we'd like to figure out which edges specifically belong on our circuit for this task. One way to do that, as seen in the presentation, is to ablate individual edges, and observe the change with respect to the baseline. To do this, we need to first compute the baseline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uke7rId6nIcM"},"outputs":[],"source":["baseline = evaluate.evaluate_baseline(model, dataset, metric).mean()\n","print(baseline)"]},{"cell_type":"markdown","metadata":{"id":"hUf1_XI-nIcM"},"source":["Then, to ablate an individual edge, we can just remove it from the graph by setting its `in_graph` attribute to `False`. To retrive an edge, just index into `g.edges` with the name of the edge, which is `'{source}->{target}'`, where `source` and `target` are the names of the source and target nodes respectively.\n","\n","Note that the names of nodes are: `'input'`, `'logits'`, `'m{layer}'` (e.g. `m9`), and `'a{layer}.h{head}'` (e.g. `a8.h11`). So the edge from the `input` to `m9` is named `'input->m9'`, and the edge from `a8.h11` to `logits` is simply `'a8.h11->logits'`\n","\n","Note that if the target head is an attention head, the edge must also specify whether it's going into the attention head's [q]uery, [k]ey, or [v]alue inputs. So, the edge from the `input` to `a5.h5`'s query inputs is `input->a5.h5<q>`.\n","\n","In the below cells, we patch some edges that we know to be in the circuit. Based on the diagram, which other edges are important?\n","\n","<details><summary> Check out the circuit diagram here! </summary>\n","\n","![Here](https://github.com/hannamw/eacl-tutorial-resources/blob/main/images/circuit_diagram_gt41.png?raw=true)\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLq_YKN5nIcM"},"outputs":[],"source":["source = 'm10'\n","target = 'logits'\n","edge_name = f'{source}->{target}'\n","g.edges[edge_name].in_graph = False\n","patched_metric = evaluate.evaluate_graph(model, g, dataset, metric).mean()\n","g.edges[edge_name].in_graph = True\n","\n","print(f\"The difference between the patched metric and baseline is: {patched_metric - baseline}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3YD0o0MnIcM"},"outputs":[],"source":["source = 'a9.h1'\n","target = 'm9'\n","edge_name = f'{source}->{target}'\n","g.edges[edge_name].in_graph = False\n","patched_metric = evaluate.evaluate_graph(model, g, dataset, metric).mean()\n","g.edges[edge_name].in_graph = True\n","\n","print(f\"The difference between the patched metric and baseline is: {patched_metric - baseline}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQ17DI24nIcM"},"outputs":[],"source":["source = 'm0'\n","target = 'a8.h11<v>'\n","edge_name = f'{source}->{target}'\n","g.edges[edge_name].in_graph = False\n","patched_metric = evaluate.evaluate_graph(model, g, dataset, metric).mean()\n","g.edges[edge_name].in_graph = True\n","\n","print(f\"The difference between the patched metric and baseline is: {patched_metric - baseline}\")"]},{"cell_type":"code","source":["source = None  # Replace this with a source node from the circuit diagram\n","target = None  # Replace this with a target node from the circuit diagram\n","edge_name = f'{source}->{target}'\n","g.edges[edge_name].in_graph = False\n","patched_metric = evaluate.evaluate_graph(model, g, dataset, metric).mean()\n","g.edges[edge_name].in_graph = True\n","\n","print(f\"The difference between the patched metric and baseline is: {patched_metric - baseline}\")"],"metadata":{"id":"AatMjcmYa3rB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LMANBdR4nIcN"},"source":["### 3.2 Scoring our Circuit via Edge Attribution Patching\n","\n","We could continue to see how important each individual edge is, and choose to remove it from the circuit if not important enough, but this would be both tedious and time-consuming. Moreover, even automating this would be rather slow. Instead, we'll figure out what edges should be in our circuit using another new technique: Edge Attribution Patching.\n","\n","Edge Attribution Patching (EAP; [Nanda (2023)](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching), [Syed et al. (2023)](https://arxiv.org/abs/2310.10348)) is a technique that approximates what happens during edge patching. In particular, this is what a normal graph on clean and corrupted inputs look like:\n","![eap1](https://github.com/hannamw/eacl-tutorial-resources/blob/main/images/eap_diagram1.png?raw=true)\n","\n","This is what we do during edge patching:\n","\n","![eap2](https://github.com/hannamw/eacl-tutorial-resources/blob/main/images/eap_diagram2.png?raw=true)\n","\n","This can be approximated in the following fashion:\n","\n","![eap3](https://github.com/hannamw/eacl-tutorial-resources/blob/main/images/eap_diagram3.png?raw=true)\n","\n","This is implemented below in the attribute method! It'll score each edge, trying to approximate how much the metric would change if that edge were patched - without actually patching it!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i0X1cl5JnIcN"},"outputs":[],"source":["attribute.attribute(model, g, dataset, partial(metric, loss=True, mean=True))"]},{"cell_type":"markdown","metadata":{"id":"Ni2rxCYTbFai"},"source":["### 3.3 Finding our Circuit\n","EAP assigns each edge in our graph a score. However, it doesn't actually give us a circuit; it just tells us how important it thinks each edge is to the circuit. To choose which edges you want to include, you can use `apply_threshold(threshold)`, which includes all edges whose value is over the given threshold.\n","\n","You can also call `apply_greedy(n)`, which greedily searches for the `n` best edges to add! We recommend this."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YgVlQqahnIcN"},"outputs":[],"source":["# include all edges whose absolute score is >= the 400th greatest absolute score\n","scores = g.scores(absolute=True)\n","g.apply_threshold(scores[-400], absolute=True)\n","\n","# using a greedy search over the graph, starting from the logits, add in the 400 highest-scoring edges (non-absolute)\n","g.apply_greedy(400)"]},{"cell_type":"markdown","metadata":{"id":"sRTO6tvsnIcN"},"source":["Sometimes these circuit finding methods might leave circuits with no parents, or no children. This doesn't make sense; neither such node should exist in a circuit. In these cases, we can prune them by calling `prune_dead_nodes`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xq3u1Wq5nIcN"},"outputs":[],"source":["g.prune_dead_nodes()"]},{"cell_type":"markdown","metadata":{"id":"FAeBFLPlnIcO"},"source":["### 3.4 Visualizing and Saving our Circuit\n","\n","We can now visualize and save our circuit!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c6DiVPV_nIcO"},"outputs":[],"source":["gz = g.to_graphviz()\n","gz.draw('graph.png', prog='dot')\n","Image(filename='graph.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FgaQt3snIcO"},"outputs":[],"source":["g.to_json('graph.json')"]},{"cell_type":"markdown","metadata":{"id":"-rM9HYSGnIcO"},"source":["### 3.5 Evaluating our Circuit\n","\n","Finally, we can evaluate our circuit! How faithful is it to our model, anyway? It's easy to test using `evaluate_graph`, which ablates all edges not in the graph."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmSgHpdwnIcO"},"outputs":[],"source":["results = evaluate.evaluate_graph(model, g, dataset, metric).mean()\n","print(f\"baseline performance: {baseline}. circuit performance: {results}\")"]},{"cell_type":"markdown","metadata":{"id":"-i3Y31f7nIcO"},"source":["### 3.5 Evaluating a Hand-Found Circuit\n","What if we already have a circuit in mind that we want to test? We can evaluate one of those in this framework too! For example, take the greater-than circuit that was found manually. Does it work? See below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g75Tyx7InIcW"},"outputs":[],"source":["g_manual = Graph.from_model(model)\n","for edge in g_manual.edges.values():\n","    edge.in_graph=False\n","\n","input_node = g_manual.nodes['input']\n","low_attn = ['a0.h3', 'a0.h5']\n","low_mlps = ['a0.h1', 'm0', 'm1', 'm2', 'm3']\n","mid_attn = ['a5.h1', 'a5.h5','a6.h1','a6.h9', 'a7.h10', 'a8.h8', 'a8.h11', 'a9.h1']\n","high_mlps = [f'm{i}' for i in range(8,12)]\n","logit_node = g_manual.nodes['logits']\n","\n","for node in low_attn + low_mlps:\n","    if f'input->{node}' in g_manual.edges:\n","        g_manual.edges[f'input->{node}'].in_graph = True\n","    else:\n","        for letter in 'qkv':\n","            g_manual.edges[f'input->{node}<{letter}>'].in_graph = True\n","\n","    for node2 in mid_attn:\n","        for letter in 'qkv':\n","            g_manual.edges[f'{node}->{node2}<{letter}>'].in_graph = True\n","\n","for i, node in enumerate(low_mlps):\n","    for node2 in low_mlps[i+1:]:\n","        g_manual.edges[f'{node}->{node2}'].in_graph = True\n","\n","for node in mid_attn:\n","    g_manual.edges[f'{node}->logits'].in_graph=True\n","    for node2 in high_mlps:\n","        edge_str = f'{node}->{node2}'\n","        if edge_str in g_manual.edges:\n","            g_manual.edges[edge_str].in_graph=True\n","\n","for i, node in enumerate(high_mlps):\n","    g_manual.edges[f'{node}->logits'].in_graph=True\n","    for node2 in high_mlps[i+1:]:\n","        g_manual.edges[f'{node}->{node2}'].in_graph = True\n","g_manual.prune_dead_nodes(prune_childless=True, prune_parentless=True)\n","results = evaluate.evaluate_graph(model, g_manual, dataset, metric).mean()\n","print(f\"baseline performance: {baseline}. circuit performance: {results}\")"]},{"cell_type":"markdown","metadata":{"id":"k_QLI9tWnIcW"},"source":["The performance is worse, but the circuit is much smaller! You can go back and test how well an automatically found circuit of equivalent size would be. You can also look at the manual circuit."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RKG7azB1nIcW"},"outputs":[],"source":["print(f\"The auto-circuit has {g.count_included_edges()}, while the manual circuit has {g_manual.count_included_edges()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfNHPLdKnIcW"},"outputs":[],"source":["for edge in g_manual.edges.values():\n","    if edge.score is None:\n","        edge.score = 0\n","gz_manual = g_manual.to_graphviz()\n","gz_manual.draw('graph_manual.png', prog='dot')\n","Image(filename='graph_manual.png')"]},{"cell_type":"markdown","metadata":{"id":"ZEN7SlNFnIcW"},"source":["## 4 Play around with Circuits\n","Congrats on making it through this notebook! Now you can try playing with circuits on your own. You can try, for example:\n","- using a different number of edges in the circuit\n","- using a different model\n","- creating a different task\n","\n","If you're interested, there's also another kind of attribution that can be activated by setting running `attribute.attribute(model, g, dataset, partial(metric, loss=True, mean=True), integrated_gradients=30)`. This doesn't work well on the greater-than task, but works very well on some attention-head-heavy tasks where EAP normally fails. This is EAP with [integrated gradients](https://arxiv.org/abs/1703.01365); see my upcoming paper for more details!"]},{"cell_type":"markdown","metadata":{"id":"q3v-6suPxWEt"},"source":["## 5. Bonus\n","\n","Congratulations on finishing this notebook! Mechanistic interpretability is a large field, though, and we've only scratched the surface of it. Here are some interesting lines of work that you might want to look into when you're done with this.\n","\n","- Circuits work:\n","    - in real language models:\n","        - [IOI](https://arxiv.org/abs/2211.00593)\n","        - [Greater-Than](https://arxiv.org/abs/2305.00586)\n","        - [Entity Tracking](https://openreview.net/forum?id=8sKcAWOf2D)\n","        - [Sentiment](https://arxiv.org/abs/2310.15154)\n","        - [QA in Chinchilla](https://arxiv.org/abs/2307.09458)\n","        - [Circuit Component Re-use](https://arxiv.org/abs/2310.08744)\n","    - in toy models:\n","        - [Grokking modular addition](https://openreview.net/forum?id=9XFSbDPmdW)\n","        - [Docstring circuit](https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only)\n","- Automated circuit finding work:\n","    - [Automated Circuit Discovery](https://arxiv.org/abs/2304.14997)\n","    - Edge Attribution Patching:\n","        - [original blog post](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching)\n","        - [paper](https://arxiv.org/abs/2310.10348)\n","        - [AtP*, a follow-up](https://arxiv.org/abs/2403.00745)\n","        - EAP-IG (coming soon!)\n","- Studies of individual components:\n","    - [Successor Heads](https://openreview.net/pdf?id=kvcbV8KQsi)\n","    - [Copy Suppression](https://arxiv.org/abs/2310.04625)\n","- Mechanistic Interpretability Methods:\n","    - [Path Patching](https://arxiv.org/abs/2304.05969)\n","    - [Subspace Patching Illusions](https://arxiv.org/abs/2311.17030) and this [rebuttal](https://arxiv.org/abs/2401.12631)\n","    - [Best Practices in Activation Patching](https://openreview.net/forum?id=Hf17y6u9BC)\n","    - [Causal Scrubbing](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing)\n","- Anthropic's work on mech. interp.:\n","    - [A Mathematical Framework for Circuits](https://www.anthropic.com/index/a-mathematical-framework-for-transformer-circuits), and a [recent update](https://www.anthropic.com/index/circuits-updates-may-2023)\n","    - [Induction Heads](https://www.anthropic.com/index/in-context-learning-and-induction-heads)\n","    - [Superposition](https://www.anthropic.com/index/toy-models-of-superposition) and [SoLU](https://www.anthropic.com/index/softmax-linear-units)\n","    - [Privileged Bases in the Residual Stream](https://www.anthropic.com/index/privileged-bases-in-the-transformer-residual-stream)\n","\n","- Other cool (mostly) mechanistic work:\n","    - [ROME](https://arxiv.org/pdf/2202.05262.pdf) and this [follow-up](https://arxiv.org/pdf/2301.04213.pdf), plus more on fact retrieval: [paper 1](https://arxiv.org/abs/2304.14767) [paper 2](https://openreview.net/forum?id=P2gnDEHGu3) [paper 3](https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall)\n","    - [Interpreting a model trained to play Othello](https://arxiv.org/abs/2210.13382) and this [follow up](https://www.neelnanda.io/mechanistic-interpretability/othello).\n","    - The [\"hydra effect\"](https://arxiv.org/abs/2307.15771)\n","    - Chris Olah's [blog posts](https://distill.pub/2020/circuits/) on circuits in ConvNets\n","\n","If you're looking for more notebooks or practice with TransformerLens, have a look at these:\n","- [Original Demo Notebook](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Main_Demo.ipynb)\n","- [Intro to mechanistic interpretability (from ARENA)](https://arena3-chapter1-transformer-interp.streamlit.app/)"]},{"cell_type":"code","source":[],"metadata":{"id":"kbffqN0Im3i9"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}