{"cells":[{"cell_type":"markdown","metadata":{"id":"IVeslKqjbFZf"},"source":["# Transformer-Specific Interpretability: Mechanistic Interpretability\n","**Michael Hanna**\n","\n","**Notebook Credits:**\n","This notebook builds on a notebook used in the IEinAI'23 mechanistic interpretability workshop, created by Michael Hanna and Oskar van der Wal. That notebook was in turn based on a [demo notebook](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Main_Demo.ipynb), by Neel Nanda, creator of TransformerLens.\n","\n","---\n","⚠️**Before starting this notebook:**⚠️\n","- [ ] Change the runtime to GPU (`Runtime -> Change runtime type -> Hardware Accelator -> GPU`)."]},{"cell_type":"markdown","metadata":{"id":"qXn58UFrGldW"},"source":["## 0 Setup\n","Just run this code -- don't bother reading it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SbVqGa7tbFZ3"},"outputs":[],"source":["%pip install transformer-lens\n","!wget https://raw.githubusercontent.com/hannamw/eacl-tutorial-resources/main/files/greater-than-data.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPrmWV0nbFaC"},"outputs":[],"source":["# Import stuff\n","import torch\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","import plotly.io as pio\n","\n","from typing import List, Union, Optional, Tuple, Literal\n","from functools import partial\n","\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer, ActivationCache\n","\n","pio.renderers.default = \"colab\"\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","if not torch.cuda.is_available():\n","    print(\"WARNING: Running on CPU. Did you remember to set your Colab accelerator to GPU?\")\n","torch.set_grad_enabled(False)"]},{"cell_type":"markdown","metadata":{"id":"Evm3wFrAbFaK"},"source":["## 1 Models in TransformerLens\n","\n","This tutorial is built with [TransformerLens](https://github.com/neelnanda-io/TransformerLens), a powerful interpretability library for working with pre-trained transformer-based NLP models. TransformerLens allows you to easily interact with model representations and components at a low-level. For example, you can:\n","- run a forward / backward pass on the model and save attention head outputs, attention patterns, neuron activations, etc.\n","- intervene on the model during a forward pass, selecting a specific component (attention head, MLP, neuron) and changing its inputs or outputs.\n","\n","The second point is crucial, as mechanistic interpretability often relies on **causal interventions**: making changes to model internals, and observing changes in model behavior. After we develop a hypothesis, we verify it by making targeted changes to our model, and ensuring that the corresponding behavioral changes are what we expect.\n","\n","TransformerLens is implemented in PyTorch, so models should feel similar to those you have used before. So far, it supports primarily autoregressive models (e.g., GPT-2). Models must be ported to TransformerLens before use."]},{"cell_type":"markdown","metadata":{"id":"hCmpGrtXbFaM"},"source":["### Loading and Running Models\n","To begin, we will simply load and run a pre-trained model, `gpt2-small`. By setting `model_name` to other values, you can load other models (but this notebook is built for `gpt2-small`). See other valid model names [here](https://neelnanda-io.github.io/TransformerLens/model_properties_table.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtDarERlbFaP"},"outputs":[],"source":["model_name = 'gpt2-small'\n","model = HookedTransformer.from_pretrained(model_name, device=device)\n","model.cfg.use_attn_in = True\n","model.cfg.use_split_qkv_input = True\n","model.cfg.use_attn_result = True\n","model.cfg.use_hook_mlp_in = True"]},{"cell_type":"markdown","metadata":{"id":"gkfzunIjbFaQ"},"source":["Models in TransformerLens run very similarly to HuggingFace Transformers models. Unlike HuggingFace models, however, TransformerLens' HookedTransformers bundle together models and tokenizers. You can access the tokenizer via `model.tokenizer`, but you can also call `model()` directly on strings (or on tokens)!\n","\n","Try this for yourself, setting `s` to a value of your choice."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sr37wX3LbFaU"},"outputs":[],"source":["s = None  #  Change this to a sentence (fragment) of your choice, to see what the next tokens are!\n","logits = model(s).squeeze(0).cpu()\n","probs = torch.softmax(logits, dim=-1)\n","\n","# let's see what the top 5 predictions are\n","probs, next_tokens = torch.topk(probs[-1], 5)\n","print(s, \"...\")\n","for token_id, prob in zip(next_tokens, probs):\n","    token = model.tokenizer.decode(token_id.item())\n","    print(f\"{token.strip()}\\t {prob.item():.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"mEtMtaF_l4l6"},"source":["## 2 The Task: Greater-Than"]},{"cell_type":"markdown","metadata":{"id":"M9O0ke9TbFaZ"},"source":["Now that we've loaded up a model, let's look for a circuit for a specific task! In this tutorial, we'll focus on the greater-than task, which has a known and faithful circuit, but you can replace this with your own as well! For other circuits work, see ..."]},{"cell_type":"markdown","metadata":{"id":"j7Qcw5nql4l6"},"source":["### 2.1 Loading and viewing the data\n","For your convenience, we've prepared a pre-made dataset for this task. Each datapoint is a clean sentence, corrupted sentence, and the start year of the event; the corrupted start year is always `01`.\n","\n","In this dataset, all sentences tokenize to the same length! It's only necessary that each pair of (clean, corrupted) tokenize to the same length, but if all sentences are the same length, you can always assess your metric on the final (-1) token position, instead of taking into account input lengths."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0qO9GN88l4l7"},"outputs":[],"source":["df = pd.read_csv('greater-than-data.csv')\n","print(df)\n","print(df.iloc[0])"]},{"cell_type":"markdown","metadata":{"id":"2f_tiWuBbFab"},"source":["### 2.2 Measuring Performance\n","We will measure performance using probability difference (`prob_diff`). This measures the probability assigned to correct years minus the probability assigned to incorrect year. So, for a sentence like \"The journey lasted from the year 1577 to the year 15\", we would measure the probability assigned to years > 77, minus the probability assigned to years <= 77."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y86GdZy6l4l7"},"outputs":[],"source":["year_indices = torch.tensor([model.tokenizer(f'{year:02d}').input_ids[0] for year in range(100)])\n","\n","def prob_diff(logits: torch.Tensor, labels: torch.Tensor):\n","    probs = torch.softmax(logits[:, -1], dim=-1)\n","    probs = probs[:, year_indices]\n","\n","    results = []\n","    for prob, year in zip(probs, labels):\n","        results.append(prob[year + 1 :].sum() - prob[: year + 1].sum())\n","\n","    results = torch.stack(results)\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"Ni2rxCYTbFai"},"source":["### 2.3 Behavioral Evaluation\n","Before we start to find a circuit for this task, we should know how our model performs on it! So, we'll measure its peformance according to our metric, across the entire dataset. We'll do this for the clean and corrupted inputs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rOmn4Q2Jl4l7"},"outputs":[],"source":["def batch_dataset(df, batch_size=8):\n","    clean, corrupted, label = [df[col].tolist() for col in ['clean', 'corrupted', 'start_year']]\n","    clean = [clean[i:i+batch_size] for i in range(0, len(df), batch_size)]\n","    corrupted = [corrupted[i:i+batch_size] for i in range(0, len(df), batch_size)]\n","    label = [torch.tensor(label[i:i+batch_size]) for i in range(0, len(df), batch_size)]\n","    return [(clean[i], corrupted[i], label[i]) for i in range(len(clean))]\n","\n","dataset = batch_dataset(df, batch_size=8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TCk7LnFl4l7"},"outputs":[],"source":["baseline, corrupted_baseline = [], []\n","for clean, corrupted, labels in dataset:\n","    clean_logits = model(clean).cpu()\n","    baseline.append(prob_diff(clean_logits, labels))\n","\n","    corrupted_logits = model(corrupted).cpu()\n","    corrupted_baseline.append(prob_diff(corrupted_logits, labels))\n","\n","baseline = torch.cat(baseline).mean()\n","corrupted_baseline = torch.cat(corrupted_baseline).mean()\n","\n","print(f\"Baseline performance: {baseline:.2f}; corrupted performance: {corrupted_baseline:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"wMtgSt-Fl4l8"},"source":["Great! Our model performs well given clean inputs, and quite poorly given corrupted ones. Let's take one last look at its performance on a specific example. We'll see how the model correctly assigns input only to years > the start year.\n","\n","You can explore different examples by changing the value of `i`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-umCIkrl4l8"},"outputs":[],"source":["i = 0\n","print(df['clean'][i])\n","year_probs = torch.softmax(model(df['clean'][i]).squeeze(0)[-1], dim=-1)[year_indices].cpu()\n","plt.plot(np.arange(100), year_probs)\n","plt.ylabel('Probability')\n","plt.xlabel('Year')\n","plt.title(f\"Probability vs. year for start year = {df['start_year'][i]}\")"]},{"cell_type":"markdown","metadata":{"id":"1IwCS79gbFak"},"source":["## 3 Interventions for Circuit Finding\n","In this section, we'll try to understand how GPT-2 implements greater-than using causal interventions. We'll ablate components and edges in order to see how they contribute to the performance of this task."]},{"cell_type":"markdown","metadata":{"id":"4VfL2aygbFan"},"source":["### 3.1 Preliminary: How to save and intervene on model components\n","\n","In order to intervene on model components, TransformerLens introduces `HookPoint`s. A `HookPoint` is an object that wraps the output of a given model component, allowing you to interact with the model's activations. For example, `HookPoint`s exist at the inputs and outputs of each MLP and attention layer, as well as at many intermediate stages thereof.\n","\n","You can make changes to model activations at a given `HookPoint` by registering a `hook_fn` to it. A `hook_fn` is a function that takes in the activations wrapped by the `HookPoint`, and optionally outputs new values for the activations. During the model's forward pass, the `hook_fn` is called on the activations wrapped by the `HookPoint`; if the `hook_fn` returns a value, this is used as the new value of the activation. In this way, a `hook_fn` can save activations, or intervene on them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGsN5hg8l4l8"},"outputs":[],"source":["def zero_ablation_hook(activations: torch.Tensor, hook: HookPoint) -> torch.Tensor:\n","    \"\"\"\n","    Zeroes out activations at a given HookPoint\n","    \"\"\"\n","    return torch.zeros_like(activations)\n","\n","activation_cache = {}\n","def save_activation_hook(activations: torch.Tensor, hook: HookPoint) -> torch.Tensor:\n","    \"\"\"\n","    Saves activations in a cache but does not intervene on them\n","    \"\"\"\n","    activation_cache[hook.name] = activations.detach()"]},{"cell_type":"markdown","metadata":{"id":"DC7uXfPJl4l8"},"source":["Each `HookPoint` has a name, telling you where on the model it refers to. We will start by manipulating the outputs of attention heads and MLPs, denoted by `blocks.n.attn.hook_result` and `blocks.n.hook_mlp_out` respectively, where `n` is the layer of the heads / MLP. Note that the activation at `blocks.n.attn.hook_result` contains output for all attention heads, and has size `[batch, position, head, d_model]`; `blocks.n.hook_mlp_out` has size `[batch, position, d_model]`.\n"]},{"cell_type":"markdown","metadata":{"id":"0itJ45PLl4l8"},"source":["<details>\n","<summary> Click to show a diagram with the names and locations of the `HookPoint`s in a typical TransformerLens HookedTransformer model.</summary>\n","\n","![big_image](https://github.com/hannamw/eacl-tutorial-resources/blob/main/images/gpt2-diagram-annotated.png?raw=true)\n","\n","**Figure 1:** Left: Schematic of a Transformer model. Middle: More detailed schematic of the Transformer Block. Right: Names of the components in TransformerLens, where the letters (`a`,`b`,`c`, etc.) refer to the annotated components in images on the left. Image adapted from [Wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Full_GPT_architecture.png/800px-Full_GPT_architecture.png).\n","</details>\n"]},{"cell_type":"markdown","metadata":{"id":"4n4-WvKbbFbZ"},"source":["### 3.2 Simple Ablations\n","Now, we'll learn how to do some of the simpler ablations we learned about, and use them to determine which components are important to the greater-than task."]},{"cell_type":"markdown","metadata":{"id":"bkqYb1KYbFbZ"},"source":["#### Zero Ablation\n","For our first ablation, we'll just set the entire MLP / attention head's activation to 0. We'll do this either at all positions (`slice(None, None, None)` is like indexing with `[:]`), or at one of our choice; you can change the value of `position`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2nSXhvGNbFba"},"outputs":[],"source":["position = slice(None, None, None)\n","def zero_ablation_hook(activations: torch.Tensor, hook: HookPoint, head_to_ablate=Union[int, Literal['mlp']]) -> torch.Tensor:\n","    if head_to_ablate == 'mlp':\n","        activations[:, position] = 0.\n","    else:\n","        activations[:, position, head_to_ablate] = 0.\n","    return activations\n","\n","def make_zero_ablation_hook(layer: int, head_to_ablate=Union[int, Literal['mlp']]):\n","    hook_name = f'blocks.{layer}.hook_mlp_out' if head_to_ablate == 'mlp' else f'blocks.{layer}.attn.hook_result'\n","    return hook_name, partial(zero_ablation_hook, head_to_ablate=head_to_ablate)"]},{"cell_type":"markdown","metadata":{"id":"BdGUg0V4l4l9"},"source":["Now, try it out! You must select the layer / head that you want to zero ablate, and see how that affects performance. What might be interesting to ablate?\n","\n","<details><summary> Check the circuit diagram here! </summary>\n","\n","![Here](https://github.com/hannamw/eacl-tutorial-resources/blob/main/images/circuit_diagram_gt41.png?raw=true)\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzNCMqUgl4l9"},"outputs":[],"source":["layer = None  # Set this to a layer index (0-11)\n","head = None  # Set this to a head index (0-11) or to 'mlp'\n","results = []\n","for clean, corrupted, labels in dataset:\n","    cache = model.run_with_cache(corrupted)\n","    # ablate component\n","    logits = model.run_with_hooks(clean, fwd_hooks=[make_zero_ablation_hook(layer, head)]).cpu()\n","\n","    # compute metric on ablated logits\n","    result = prob_diff(logits, labels)\n","    results.append(result)\n","\n","results = torch.cat(results).mean()\n","\n","print(f\"Baseline prob diff was {baseline:0.2f}. Ablating {(layer, head)} yields prob diff {results:.2f}.\")"]},{"cell_type":"markdown","metadata":{"id":"ZT7nUaBQl4l-"},"source":["What if we try ablating each head and MLP, one at a time? Iterating over layers and heads / MLP, let's apply a zero ablation to each one. We'll record the prob diff achieved, and subtract the baseline from that; this will tell us how much the ablation caused performance to drop."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"va1WkoMdl4l-"},"outputs":[],"source":["all_prob_diffs = torch.zeros((12, 13))\n","total = 0\n","for clean, corrupted, labels in dataset[:10]:\n","    total += len(clean)\n","    for layer in range(12):\n","        for j, head in enumerate([*range(12), 'mlp']):\n","            logits = model.run_with_hooks(clean, fwd_hooks=[make_zero_ablation_hook(layer, head)]).cpu()\n","            metric = prob_diff(logits, labels)\n","            all_prob_diffs[layer,j] += metric.sum()\n","\n","all_prob_diffs /= total\n","prob_change = all_prob_diffs - baseline\n","\n","px.imshow(prob_change, color_continuous_scale='RdBu', labels={'x': 'head', 'y': 'layer', 'color':'prob diff'}, zmax=0.35, zmin=-0.35, y=list(range(12)), x=[str(c) for c in [*range(12), 'mlp']], width=800, height=600, title=f'Prob Diff by layer / head')"]},{"cell_type":"markdown","metadata":{"id":"rbhIVyMlbFbd"},"source":["#### Component-Level Activation Patching\n","Now, we'll perform activation patching at the component (not edge) level. To do so, we need to store the activations of our components from another set of inputs; for example, the corrupted inputs, which we know will cause our model to behave very differently.\n","\n","Instead of writing our own `hook_fn` for this, we'll use the built-in `model.run_with_cache()` method, which returns a dictionary mapping from `HookPoint.name`s to the activation at that `HookPoint`. It saves all `HookPoint`s at the same time; inefficient, but convenient!\n","\n","After saving a component's corrupted activation, we can set that component's activation during the clean pass, to its corrupted version."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPd0NZbKbFbe"},"outputs":[],"source":["position = slice(None, None, None)\n","\n","def replacement_ablation_hook(activations: torch.Tensor, hook: HookPoint, head: Union[int, Literal['mlp']], cache: ActivationCache) -> torch.Tensor:\n","    if head == 'mlp':\n","        activations[:, position] = cache[hook.name][:, position]\n","    else:\n","        activations[:, position, head] = cache[hook.name][:, position, head]\n","    return activations\n","\n","def make_replacement_ablation_hook(layer: int, head: Union[int, Literal['mlp']], cache: ActivationCache):\n","    hook_name = f'blocks.{layer}.hook_mlp_out' if head == 'mlp' else f'blocks.{layer}.attn.hook_result'\n","    return hook_name, partial(replacement_ablation_hook, head=head, cache=cache)"]},{"cell_type":"markdown","metadata":{"id":"WenjtzGel4l-"},"source":["Again, it's now your turn to play with replacment ablations! Try it out - are the results the same as or different from those of the zero ablations?\n","\n","<details><summary> Find the circuit diagram here </summary>\n","\n","![Here](https://github.com/hannamw/eacl-tutorial-resources/blob/main/images/circuit_diagram_gt41.png?raw=true)\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRn05avhl4l-"},"outputs":[],"source":["layer = None  # Set this to a layer index (0-11)\n","head = None  # Set this to a head index (0-11) or to 'mlp'\n","results = []\n","for clean, corrupted, labels in dataset:\n","    # run with cache, discarding first returned item (corrupted_logits)\n","    _, cache = model.run_with_cache(corrupted)\n","    # ablate component\n","    logits = model.run_with_hooks(clean, fwd_hooks=[make_replacement_ablation_hook(layer, head, cache)]).cpu()\n","\n","    # compute metric on ablated logits\n","    result = prob_diff(logits, labels)\n","    results.append(result)\n","\n","results = torch.cat(results).mean()\n","\n","print(f\"Baseline prob diff was {baseline:0.2f}. Ablating {(layer, head)} yields prob diff {results:.2f}.\")"]},{"cell_type":"markdown","metadata":{"id":"HJOxBscxl4l-"},"source":["Again, let's do this iteratively!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slIbT9Yil4mC"},"outputs":[],"source":["all_prob_diffs = torch.zeros((12, 13))\n","total = 0\n","for clean, corrupted, labels in dataset[:5]:\n","    total += len(clean)\n","    _, cache = model.run_with_cache(corrupted)\n","    for layer in range(12):\n","        for j, head in enumerate([*range(12), 'mlp']):\n","            logits = model.run_with_hooks(clean, fwd_hooks=[make_replacement_ablation_hook(layer, head, cache)]).cpu()\n","            metric = prob_diff(logits, labels)\n","            all_prob_diffs[layer,j] += metric.sum()\n","\n","all_prob_diffs /= total\n","prob_change = all_prob_diffs - baseline\n","\n","px.imshow(prob_change, color_continuous_scale='RdBu', labels={'x': 'head', 'y': 'layer', 'color':'prob diff'}, y=list(range(12)), x=[str(c) for c in [*range(12), 'mlp']], width=800, height=600, title=f'Prob Diff by layer / head', zmax=0.5, zmin=-0.5)"]},{"cell_type":"markdown","metadata":{"id":"oLm6NRqll4mC"},"source":["#### Edge-Level Activation Patching\n","Now, we do the same thing, but for edges! This makes things slightly more complicated. Rather than ablating the output of an individual node, we have two nodes, the sending and receiving node. We must reconstruct the **input** of the receiving node, subtracting out the sending node's clean activation, and adding in its corrupted activation.\n","\n","This means we have to consider new `HookPoint`s: hooks that refer to the **input** of each component. For example, the hook into an MLP's input is `blocks.n.hook_mlp_in`. In contrast, you can hook into attention head inputs via `blocks.n.hook_attn_in`, or hook into the input of the queries / keys / values directly via `blocks.n.attn.hook_[q/k/v]_in`.\n","\n","In this exercise, we'll start by considering the hooks that end in the logits / unembedding. There's actually no dedicated `HookPoint` for this, but the input to the unembedding is the same as the residual stream output of the entire last transformer layer, `blocks.11.hook_resid_post`; the 11 comes from the fact that GPT-2 small has 11 layers.\n","\n","We'll perform the input reconstruction by first creating corrupt and clean caches and then doing the activation patching, but note that you can actually gather the clean inputs in a cache and use them to perform activation patching in the same forward pass."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMGLWzL9l4mD"},"outputs":[],"source":["position = slice(None, None, None)\n","\n","def logit_edge_hook(activations: torch.Tensor, hook: HookPoint, output_layer: int, output_head: Union[int, Literal['mlp']],\n","              corrupted_cache: ActivationCache, clean_cache: ActivationCache) -> torch.Tensor:\n","    output_hook_name = f'blocks.{output_layer}.hook_mlp_out' if output_head == 'mlp' else f'blocks.{output_layer}.attn.hook_result'\n","\n","    # construct the output index: if we're patching the output of an attention head, index in as [:, position, output_head]\n","    # otherwise, index in as [:, position]\n","    index = (slice(None, None, None), position, output_head) if isinstance(output_head, int) else (slice(None, None, None), position)\n","\n","    activations[:, position] -= clean_cache[output_hook_name][index]\n","    activations[:, position] += corrupted_cache[output_hook_name][index]\n","    return activations\n","\n","def make_logits_edge_hook(layer: int, head: Union[int, Literal['mlp']], corrupted_cache: ActivationCache,\n","                          clean_cache: ActivationCache):\n","    input_hook_name = 'blocks.11.hook_resid_post'\n","    return input_hook_name, partial(logit_edge_hook, output_layer=layer, output_head=head, corrupted_cache=corrupted_cache, clean_cache=clean_cache)\n","\n","all_prob_diffs = torch.zeros((12, 13))\n","total = 0\n","for clean, corrupted, labels in dataset[:5]:\n","    total += len(clean)\n","    _, corrupted_cache = model.run_with_cache(corrupted)\n","    _, clean_cache = model.run_with_cache(clean)\n","    for layer in range(12):\n","        for j, head in enumerate([*range(12), 'mlp']):\n","            logits = model.run_with_hooks(clean, fwd_hooks=[make_logits_edge_hook(layer, head, corrupted_cache, clean_cache)]).cpu()\n","            metric = prob_diff(logits, labels)\n","            all_prob_diffs[layer, j] += metric.sum()\n","\n","all_prob_diffs /= total\n","prob_change = all_prob_diffs - baseline\n","\n","px.imshow(prob_change, color_continuous_scale='RdBu', labels={'x': 'head', 'y': 'layer', 'color':'prob diff'}, y=list(range(12)), x=[str(c) for c in [*range(12), 'mlp']], width=800, height=600, title=f'Prob Diff by layer / head', zmax=0.3, zmin=-0.3)"]},{"cell_type":"markdown","metadata":{"id":"pQ0GSIbxl4mD"},"source":["Great! Now we know which nodes contribute directly to the logits. This is the first step of finding the circuit responsible for greater-than.\n","\n","<details><summary>It corresponds to this image/step from the tutorial</summary>\n","\n","![logits_patching](https://github.com/hannamw/eacl-tutorial-resources/blob/main/images/logits_patching.png?raw=true)\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"XLc8JDKpBme9"},"source":["### 4 Finding the Greater-Than Circuit, Manually\n","\n","If you've made it this far, you're ready to find the rest of the greater-than circuit by hand! This section of the notebook will walk you through the first steps of finding important nodes in a principled way: using edge patching. It'll then show you how to characterize what each component does, via the logit lens. For circuit faithfulness evaluation, take a look at the other notebook; it's much easier to do using a framework that handles things for you, rather than crafting functions ad-hoc as done here.\n","\n","<details><summary>If you're ever confused about what connections you should be finding, feel free to refer back to this greater-than circuit diagram!</summary>\n","\n","![circuit_diagram](https://github.com/hannamw/eacl-tutorial-resources/blob/main/images/circuit_diagram_gt41.png?raw=true)\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"wiVzFVMwnP39"},"source":["#### 4.1 Finding More Circuit Components\n","Here, we implement a more general version of logit edge patching, which patches an edge between any two components. For simplicity, we consider the edge from a node to an attention to connect to all of the attention head's inputs (Q, K, and V), but note that this isn't necessarily the right choice. After all, some nodes might only really matter to one of its Q, K, or V inputs, but not the others."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FR14UXUsl4mD"},"outputs":[],"source":["position = slice(None, None, None)\n","def component2name(layer:int, head:Union[int, Literal['mlp'], Literal['logit']], in_out:Union[Literal['in'],Literal['out']]):\n","    if in_out == 'in':\n","        if isinstance(head, int):\n","            return f'blocks.{layer}.hook_attn_in'\n","        elif head == 'mlp':\n","            return f'blocks.{layer}.hook_mlp_in'\n","        elif head == 'logit':\n","            return 'blocks.11.hook_resid_post'\n","        else:\n","            raise ValueError(f\"got bad head: {head}\")\n","    elif in_out == 'out':\n","        if isinstance(head, int):\n","            return f'blocks.{layer}.attn.hook_result'\n","        elif head == 'mlp':\n","            return f'blocks.{layer}.hook_mlp_out'\n","        elif head == 'logit':\n","            raise ValueError(\"logit has no 'out'\")\n","        else:\n","            raise ValueError(f\"got bad head: {head}\")\n","    else:\n","        raise ValueError(f\"expected in_out to be 'in' or 'out', but got {in_out}\")\n","\n","def edge_hook(activations: torch.Tensor, hook: HookPoint, target_head:Union[int, Literal['mlp'], Literal['logit']],\n","              source_layer: int, source_head: Union[int, Literal['mlp']],\n","              corrupted_cache: ActivationCache, clean_cache: ActivationCache) -> torch.Tensor:\n","\n","    source_hook_name = component2name(source_layer, source_head, 'out')\n","    # construct the indices: if we're patching the output of an attention head, index in as [:, position, output_head]\n","    # otherwise, index in as [:, position]\n","    target_index = (slice(None, None, None), position, target_head) if isinstance(target_head, int) else (slice(None, None, None), position)\n","    source_index = (slice(None, None, None), position, source_head) if isinstance(source_head, int) else (slice(None, None, None), position)\n","\n","    activations[target_index] -= clean_cache[source_hook_name][source_index]\n","    activations[target_index] += corrupted_cache[source_hook_name][source_index]\n","    return activations\n","\n","def make_edge_hook(source_layer: int, source_head: Union[int, Literal['mlp']], target_layer: int, target_head: Union[int, Literal['mlp']],\n","                   corrupted_cache: ActivationCache,  clean_cache: ActivationCache):\n","    \"\"\"\n","    Perform edge patching on the edge going from (source_layer, source_head) to (target_layer, target_head)\n","    \"\"\"\n","    target_hook_name = component2name(target_layer, target_head, 'in')\n","    return target_hook_name, partial(edge_hook, target_head=target_head, source_layer=source_layer, source_head=source_head, corrupted_cache=corrupted_cache, clean_cache=clean_cache)"]},{"cell_type":"markdown","metadata":{"id":"4h9Q8qPEl4mD"},"source":["We'll also write a new function for creating those heatmaps."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1FejeIRl4mD"},"outputs":[],"source":["def valid_source(source_layer:int, source_head: Union[int, Literal['mlp']], target_layer:int, target_head: Union[int, Literal['mlp'], Literal['logit']]):\n","    assert isinstance(source_head, int) or source_head == 'mlp'\n","    assert isinstance(target_head, int) or target_head == 'mlp' or target_head == 'logit'\n","    lower_layer = source_layer < target_layer\n","    equal_layer = source_layer == target_layer\n","    source_attn_target_not = isinstance(source_head, int) and not isinstance(target_head, int)\n","    source_mlp_target_logits = (source_head == 'mlp') and (target_head == 'logit')\n","    return lower_layer or (equal_layer and (source_attn_target_not or source_mlp_target_logits))\n","\n","def make_edge_patching_heatmap(target_layer:int, target_head: Union[int, Literal['mlp'], Literal['logit']]):\n","    \"\"\"\n","    Perform iterative edge patching, iteratively ablating the edge from every possible source component to the target layer.head\n","    Ignores \"source components\" that are downstream of the target component (and are thus not eligible source components)\n","    \"\"\"\n","    invalid = set()\n","    all_prob_diffs = torch.zeros((12, 13))\n","    total = 0\n","    for clean, corrupted, labels in dataset[:5]:\n","        total += len(clean)\n","        _, corrupted_cache = model.run_with_cache(corrupted)\n","        _, clean_cache = model.run_with_cache(clean)\n","        for layer in range(12):\n","            for j, head in enumerate([*range(12), 'mlp']):\n","                if not valid_source(layer, head, target_layer, target_head):\n","                    invalid.add((layer, j))\n","                    continue\n","                logits = model.run_with_hooks(clean, fwd_hooks=[make_edge_hook(layer, head, target_layer, target_head, corrupted_cache, clean_cache)]).cpu()\n","                metric = prob_diff(logits, labels)\n","                all_prob_diffs[layer,j] += metric.sum()\n","\n","    all_prob_diffs /= total\n","    prob_change = all_prob_diffs - baseline\n","    for i_layer, i_head in invalid:\n","        prob_change[i_layer, i_head] = 0\n","\n","    max_magnitude = prob_change.abs().max().item()\n","    fig = px.imshow(prob_change, color_continuous_scale='RdBu', labels={'x': 'head', 'y': 'layer', 'color':'prob diff'}, y=list(range(12)), x=[str(c) for c in [*range(12), 'mlp']], width=800, height=600, title=f'Iterative Path Patching into ({target_layer},{target_head})', zmax=max_magnitude, zmin=-max_magnitude)\n","    fig.show()"]},{"cell_type":"markdown","metadata":{"id":"Nrzkqra_l4mE"},"source":["Okay, now check out what contributes to nodes that are important to e.g. MLP 11, or other components connected strongly to the logits!\n","\n","<details><summary> Look at the results! Do they match the claims of the circuit diagram? </summary>\n","\n","![Here](https://github.com/hannamw/eacl-tutorial-resources/blob/main/images/circuit_diagram_gt41.png?raw=true)\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0JKM9aRul4mE"},"outputs":[],"source":["make_edge_patching_heatmap(11, 'mlp')"]},{"cell_type":"markdown","metadata":{"id":"_TSueOnQl4mE"},"source":["Similarly, you can ask what connects to nodes like a9.h1, which contribute to some nodes contributing to the logits! You can keep on doing this until you have a good idea of what the circuit looks like; we haven't written all the code necessary for it, but it's just more calls to `make_edge_patching_heatmap`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6P2hq1Yl4mE"},"outputs":[],"source":["make_edge_patching_heatmap(9, 1)"]},{"cell_type":"markdown","metadata":{"id":"YKash6wdyW6F"},"source":["#### 4.2 Characterizing Component Behavior with the Logit Lens\n","\n","For the very end, we'll try to understand what GPT-2 small's components are doing. To do so, we'll use the logit lens, a technique that multiplies component outputs by the un/embedding matrix. This takes advantage of the fact that the residual stream, which the unembedding operation transforms into logits, is just a sum of component outputs. Multiplying the component outputs and embedding matrix gives us the outputs' contributions to the model's actual logits (modulo LayerNorm and such, which TransformerLens tries to deal with; see [this](https://neelnanda-io.github.io/TransformerLens/generated/code/transformer_lens.HookedTransformer.html#:~:text=model%20properties.-,fold_ln,-%E2%80%93) for details).\n","\n","The logit lens is very simple to implement. After applying it, we'll only take those indices correponding to years, as these give the most interesting results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQrCbQbgl4mE"},"outputs":[],"source":["unembed_matrix = model.unembed.W_U\n","def year_logit_lens(activations: torch.Tensor) -> torch.Tensor:\n","    return torch.einsum('bd,dv->bv', activations, unembed_matrix)[:, year_indices]"]},{"cell_type":"markdown","metadata":{"id":"NNUTwhzsl4mE"},"source":["Now, let's apply it to individual attention heads that we know to be important!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6kYmfOWl4mE"},"outputs":[],"source":["def plot_logit_lens(example: str, layer: int, head:Union[int, Literal['mlp'], Literal['logit']], position:int=-1):\n","    index = (slice(None, None, None), position, head) if isinstance(head, int) else (slice(None, None, None), position)\n","\n","    _, cache = model.run_with_cache(example)\n","    activations = cache[component2name(layer, head, 'out')][index]\n","    year_logits = year_logit_lens(activations)[0].cpu()\n","\n","    plt.plot(np.arange(100), year_logits)\n","    plt.xlabel('year')\n","    plt.ylabel('logit')\n","    print(example)\n","    plt.show()"]},{"cell_type":"markdown","source":["Set the layer and head to those that correspond to an important attention head! Which are those?\n","\n","<details><summary> Find the circuit diagram here </summary>\n","\n","![Here](https://github.com/hannamw/eacl-tutorial-resources/blob/main/images/circuit_diagram_gt41.png?raw=true)\n","\n","</details>"],"metadata":{"id":"1r2aTK0NTZgl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"geVmaCr4l4mF"},"outputs":[],"source":["example_index = 0\n","layer = None  # Set this to a layer index (0-11)\n","head = None  # Set this to a head index (0-11) or to 'mlp'\n","plot_logit_lens(df['clean'][example_index], layer, head)"]},{"cell_type":"markdown","metadata":{"id":"TBlBpb7bl4mF"},"source":["We can also apply it to MLPs! Which MLPs are important?\n","\n","<details><summary> Find the circuit diagram here </summary>\n","\n","![Here](https://github.com/hannamw/eacl-tutorial-resources/blob/main/images/circuit_diagram_gt41.png?raw=true)\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5F7Ny_Ahl4mF"},"outputs":[],"source":["example_index = 0\n","layer = None  # Set this to a layer index (0-11)\n","head = None  # Set this to a head index (0-11) or to 'mlp'\n","plot_logit_lens(df['clean'][example_index], layer, head)"]},{"cell_type":"markdown","metadata":{"id":"Ne67-_dml4mF"},"source":["In the end, we can see how attention heads cause spikes at relevant years, while MLPs have a more general upweighting function. The latter point can be a bit hard to see; it becomes more clear if you aggregate logit lens results by year, and then plot the aggregations.\n","\n","This is the end of this notebook! Now that you're done, check out the other one."]},{"cell_type":"markdown","metadata":{"id":"q3v-6suPxWEt"},"source":["## 5. Bonus\n","\n","Congratulations on finishing this notebook! Mechanistic interpretability is a large field, though, and we've only scratched the surface of it. Here are some interesting lines of work that you might want to look into when you're done with this.\n","\n","- Circuits work:\n","    - in real language models:\n","        - [IOI](https://arxiv.org/abs/2211.00593)\n","        - [Greater-Than](https://arxiv.org/abs/2305.00586)\n","        - [Entity Tracking](https://openreview.net/forum?id=8sKcAWOf2D)\n","        - [Sentiment](https://arxiv.org/abs/2310.15154)\n","        - [QA in Chinchilla](https://arxiv.org/abs/2307.09458)\n","        - [Circuit Component Re-use](https://arxiv.org/abs/2310.08744)\n","    - in toy models:\n","        - [Grokking modular addition](https://openreview.net/forum?id=9XFSbDPmdW)\n","        - [Docstring circuit](https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only)\n","- Automated circuit finding work:\n","    - [Automated Circuit Discovery](https://arxiv.org/abs/2304.14997)\n","    - Edge Attribution Patching:\n","        - [original blog post](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching)\n","        - [paper](https://arxiv.org/abs/2310.10348)\n","        - [AtP*, a follow-up](https://arxiv.org/abs/2403.00745)\n","        - EAP-IG (coming soon!)\n","- Studies of individual components:\n","    - [Successor Heads](https://openreview.net/pdf?id=kvcbV8KQsi)\n","    - [Copy Suppression](https://arxiv.org/abs/2310.04625)\n","- Mechanistic Interpretability Methods:\n","    - [Path Patching](https://arxiv.org/abs/2304.05969)\n","    - [Subspace Patching Illusions](https://arxiv.org/abs/2311.17030) and this [rebuttal](https://arxiv.org/abs/2401.12631)\n","    - [Best Practices in Activation Patching](https://openreview.net/forum?id=Hf17y6u9BC)\n","    - [Causal Scrubbing](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing)\n","- Anthropic's work on mech. interp.:\n","    - [A Mathematical Framework for Circuits](https://www.anthropic.com/index/a-mathematical-framework-for-transformer-circuits), and a [recent update](https://www.anthropic.com/index/circuits-updates-may-2023)\n","    - [Induction Heads](https://www.anthropic.com/index/in-context-learning-and-induction-heads)\n","    - [Superposition](https://www.anthropic.com/index/toy-models-of-superposition) and [SoLU](https://www.anthropic.com/index/softmax-linear-units)\n","    - [Privileged Bases in the Residual Stream](https://www.anthropic.com/index/privileged-bases-in-the-transformer-residual-stream)\n","\n","- Other cool (mostly) mechanistic work:\n","    - [ROME](https://arxiv.org/pdf/2202.05262.pdf) and this [follow-up](https://arxiv.org/pdf/2301.04213.pdf), plus more on fact retrieval: [paper 1](https://arxiv.org/abs/2304.14767) [paper 2](https://openreview.net/forum?id=P2gnDEHGu3) [paper 3](https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall)\n","    - [Interpreting a model trained to play Othello](https://arxiv.org/abs/2210.13382) and this [follow up](https://www.neelnanda.io/mechanistic-interpretability/othello).\n","    - The [\"hydra effect\"](https://arxiv.org/abs/2307.15771)\n","    - Chris Olah's [blog posts](https://distill.pub/2020/circuits/) on circuits in ConvNets\n","\n","If you're looking for more notebooks or practice with TransformerLens, have a look at these:\n","- [Original Demo Notebook](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Main_Demo.ipynb)\n","- [Intro to mechanistic interpretability (from ARENA)](https://arena3-chapter1-transformer-interp.streamlit.app/)"]},{"cell_type":"code","source":[],"metadata":{"id":"MKkuwDUhm0n7"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}